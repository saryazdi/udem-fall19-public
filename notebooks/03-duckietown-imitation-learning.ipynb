{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils.helpers import launch_env, wrap_env, view_results_ipython, change_exercise, force_done\n",
    "from utils.helpers import SteeringToWheelVelWrapper, ResizeWrapper, ImgWrapper\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Data from a Teacher\n",
    "\n",
    "In order to use imitation learning in practice, we need to have _demonstrations_. However, demonstrations need to be gathered; in general, we can collect the demonstrations that we need in one of four ways:\n",
    "\n",
    "* Human demonstrator teleoperating the robot\n",
    "* Data logs or historical data\n",
    "* Learned policy (i.e from reinforcement learning) is rolled out\n",
    "* Hard-coded expert is rolled out\n",
    "\n",
    "While these trajectories can be gathered on real robots, to speed up collection, we work mainly in simulation. Duckietown has a [vast](https://logs.duckietown.org) **TODO Link broken** collection of logs gathered over years of running programs on Duckiebots, but here, we focus on the last data collection method: a hard-coded expert.\n",
    "\n",
    "**Question: What are some pros and cons of each approach? List two pros and two cons for each of the four methods listed above, and include it in your submission as `imitation-learning-answers.txt`**\n",
    "\n",
    "We first introduce a _pure-pursuit expert_ - often, in robotic imitation learning, we have controllers to control many of our robots and systems; a pure-pursuit expert is about the simplest controller that we can have for a Duckiebot.\n",
    "\n",
    "Our expert drives with ground-truth state data; while more complicated controllers incorporate and fuse observational data to estimate a state, we use data that'd a robot would not normally have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurePursuitExpert:\n",
    "    def __init__(self, env, ref_velocity=0.04, position_threshold=0.8, gain=10,\n",
    "                 following_distance=0.3, max_iterations=1000):\n",
    "        self.env = env.unwrapped\n",
    "        self.following_distance = following_distance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.ref_velocity = ref_velocity\n",
    "        self.gain = gain\n",
    "        self.position_threshold = position_threshold\n",
    "\n",
    "    def predict(self, observation):  \n",
    "        # Our expert drives with \"cheating\" data, something your implementation will not have access to\n",
    "        closest_point, closest_tangent = self.env.closest_curve_point(self.env.cur_pos, self.env.cur_angle)\n",
    "\n",
    "        iterations = 0\n",
    "        lookup_distance = self.following_distance\n",
    "        curve_point = None\n",
    "        while iterations < self.max_iterations:\n",
    "            # Project a point ahead along the curve tangent,\n",
    "            # then find the closest point to to that\n",
    "            follow_point = closest_point + closest_tangent * lookup_distance\n",
    "            curve_point, _ = self.env.closest_curve_point(follow_point, self.env.cur_angle)\n",
    "\n",
    "            # If we have a valid point on the curve, stop\n",
    "            if curve_point is not None:\n",
    "                break\n",
    "\n",
    "            iterations += 1\n",
    "            lookup_distance *= 0.5\n",
    "\n",
    "        # Compute a normalized vector to the curve point\n",
    "        point_vec = curve_point - self.env.cur_pos\n",
    "        point_vec /= np.linalg.norm(point_vec)\n",
    "\n",
    "        dot = np.dot(self.env.get_right_vec(), point_vec)\n",
    "        steering = self.gain * -dot\n",
    "\n",
    "        return self.ref_velocity, steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_exercise('master')\n",
    "\n",
    "local_env = launch_env()\n",
    "local_env = wrap_env(local_env)\n",
    "local_env = ResizeWrapper(local_env)\n",
    "local_env = ImgWrapper(local_env)\n",
    "\n",
    "local_env.reset()\n",
    "wrapper = SteeringToWheelVelWrapper()\n",
    "\n",
    "# Create an demonstrator\n",
    "expert = PurePursuitExpert(env=local_env)\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "\n",
    "# Collect samples\n",
    "\n",
    "for steps in range(0, nsteps):\n",
    "    # use our 'expert' to predict the next action.\n",
    "    action = expert.predict(None)\n",
    "    action = wrapper.convert(action)\n",
    "    observation, reward, done, info = local_env.step(action)\n",
    "    observations.append(observation)\n",
    "    actions.append(action)\n",
    "\n",
    "    if done: \n",
    "        local_env.reset()\n",
    "\n",
    "local_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_results_ipython(local_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: When you visualize the results, what are two major issues? Play with the expert's code and the execution code above, and list five changes that you tried, as well as their _qualitative_ effects on performance. What are some parameters that you can find to make the expert optimal (i.e cover the most distance)?**\n",
    "\n",
    "Place the answers to the above in `imitation-learning-answers.txt`, and tell us the parameters that can make the expert Duckiebot drive the furthest when you set `nsteps` to `500`. Do not reseed the environment, and report the final position that is outputted from the simulator - remember; if the episode ends before the 500 steps is over, report the final position _before_ the episode ends; to do this, you can `break` when the environment returns `done = True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Model\n",
    "\n",
    "While the above expert isn't great, it's a start. What's best is that we now have image `observations` and real-valued `actions` that we can use to train a neural network in Pytorch. Our imitation learner will driver directly from observations, and will be trained with a popular imitation learning loss: Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, action_dim, max_action):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # TODO: You'll need to change this!\n",
    "        flat_size = 0\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, stride=2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "        self.lin1 = nn.Linear(flat_size, 100)\n",
    "        self.lin2 = nn.Linear(100, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.relu(self.conv1(x)))\n",
    "        x = self.bn2(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.lr(self.lin1(x))\n",
    "\n",
    "        x = self.lin2(x)\n",
    "        x = self.max_action * self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training from the Teacher Data\n",
    "\n",
    "We can then write our _training loop_ : the piece of code that implements the process of stochastic gradient descent to minimize the loss between our network's predicted actions and those implemented by our expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 5\n",
    "batchsize = 10\n",
    "\n",
    "actions = np.array(actions)\n",
    "observations = np.array(observations)\n",
    "\n",
    "model = Model(action_dim=2, max_action=1.)\n",
    "model.train().to(device)\n",
    "\n",
    "# weight_decay is L2 regularization, helps avoid overfitting\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,\n",
    "    weight_decay=1e-3\n",
    ")\n",
    "\n",
    "avg_loss = 0\n",
    "for epoch in range(nepochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_indices = np.random.randint(0, observations.shape[0], (batchsize))\n",
    "    obs_batch = torch.from_numpy(observations[batch_indices]).float().to(device)\n",
    "    act_batch = torch.from_numpy(actions[batch_indices]).float().to(device)\n",
    "\n",
    "    model_actions = model(obs_batch)\n",
    "\n",
    "    loss = (model_actions - act_batch).norm(2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss = loss.data[0]\n",
    "    avg_loss = avg_loss * 0.995 + loss * 0.005\n",
    "\n",
    "    print('epoch %d, loss=%.3f' % (epoch, avg_loss))\n",
    "\n",
    "    # Periodically save the trained model\n",
    "    if epoch % 200 == 0:\n",
    "        torch.save(model.state_dict(), 'imitation/pytorch/models/imitate.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_done(local_env)\n",
    "local_env.reset()\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    obs = torch.from_numpy(obs).float().to(device).unsqueeze(0)\n",
    "    action = model(obs)\n",
    "    action = action.squeeze().data.cpu().numpy()\n",
    "    obs, reward, done, info = local_env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Your Submission\n",
    "\n",
    "As you may notice, the above model performs poorly. Your job is to improve it; first in the notebook, later in the AIDO submission. This last part of the assignment consists of three sections:\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "**1. Inside of your `imitation-learning-answers.txt`, qualitatively explain changes you made to both the expert and network (architecture, hyperparameters, episode lengths, number of training episodes / epochs, etc.) Be as detailed as possible, and try various experiments. You will lose points for submitting the AIDO Imitation Learning Baseline (including partial points if we find that you didn't make changes to any part of our code - hyperparameters, network, etc.)**\n",
    "\n",
    "**2. Explain some of the issues with the imitation learning loop above. Specifically, comment on the loss function and training objective. Explain, in detail, at least two issues, and give hypotheses backed by research that help solve the issues you've brought up. Did you try anything in particular that fixed these issues and improved your model?**\n",
    "\n",
    "**3. Using the instructions [here](http://docs.duckietown.org/DT19/AIDO/out/embodied_il_sim.html), use the `imitate.pt` that gets saved in this notebook and submit using the template submission provided through the AIDO submission. Report your best submission number (i.e the one you'd like to be graded) in `imitation-learning-answers.txt`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
